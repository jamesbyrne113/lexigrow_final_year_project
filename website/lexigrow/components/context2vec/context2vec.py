import nltk
import numpy as np
from nltk.stem import WordNetLemmatizer

from common.pos import nltk_to_wordnet_pos
from common.words import all_words
from .common.model_reader import ModelReader


class Context2Vec:
    lemmatizer = WordNetLemmatizer()

    model_reader = ModelReader("./resources/context2vec_model_package/model.params")
    w = model_reader.w
    word2index = model_reader.word2index
    index2word = model_reader.index2word
    model = model_reader.model

    example_test = np.array([-0.12362521, 0.00073188206, 0.04445092, -0.010664987, 0.049406514, 0.24630879, -0.056158543, -0.009991367, -0.056204055, -0.038203087, 0.016548762, -0.010539403, 0.0039436184, 0.02126962, 0.0015481304, 0.031062802, -0.0014358029, -0.009747965, -0.047745332, -0.016620133, -0.018350866, -0.044568986, 0.05036727, -0.004984747, -0.0037342834, -0.036072623, -0.00045115495, 0.010762171, 0.023506286, -0.04284772, 0.03805882, 0.0056090676, -0.0038957046, 0.026500035, 0.020466289, -0.024139421, -0.039351784, -0.022504825, 0.003728427, 0.02699353, 0.0339826, 0.122563936, -0.0037748138, -0.03798211, 0.03142079, 0.08268173, -0.023159416, 0.025127765, -0.038366806, 0.0019635838, 0.04684805, 0.020566303, -0.011505015, 0.0017476529, -0.0032344048, -0.03244152, 0.003254307, -0.0025715071, 0.009894961, -0.0302865, -0.051220696, 0.05039555, 0.06951085, 0.03519477, -0.01565689, 0.023628708, -0.016678847, 0.012504288, -0.02020718, 0.034636993, 0.0143740885, -0.0180645, 0.051658258, -0.015475696, -0.008015063, 0.007886337, 0.011831282, 0.076493435, 0.06747939, 0.045317274, 0.03608606, 0.050401386, 0.0072910315, 0.009937062, -0.022593828, 0.047077265, -0.044665303, -0.008637417, 0.008485691, 0.004939495, 0.005382322, -0.03560611, -0.0005731835, -0.025472147, 0.03863532, -0.017565146, -0.002515066, 0.025293566, -0.018992646, 0.04716808, -0.022346469, 0.029288428, -0.011012787, -0.0024141956, 0.034255296, -0.073845334, 0.03192968, -0.0019492743, 0.0028944637, -0.0098496, -0.011487297, 0.06816426, -0.026287064, 0.28711283, -0.014831204, -0.04308126, -0.05934988, -0.031215198, -0.012969224, 0.011325828, -0.034330666, -0.06493233, 0.018298667, 0.007815447, 0.0021251778, 0.025326328, -0.029610535, -0.07224439, -0.012989164, -0.010454976, 0.024351532, -0.022473058, -0.0049404143, -0.02853865, 0.007112188, 0.024882559, 0.011649786, 0.035667256, 0.00012079294, 0.004958689, 0.096189134, -0.025006386, -0.0070890053, -0.00918131, -0.025077535, 0.02313943, -0.012706001, -0.008274445, -0.036348894, 0.07128182, 0.03862267, -0.004876176, -0.0038338131, -0.007838277, 0.09037336, -0.0020142344, -0.012817628, 0.0049333805, 0.014621699, -0.0010454899, -0.007618672, 0.023260346, -0.013859629, -0.032460377, -0.006757347, 0.106617376, -0.02562225, -0.008703359, 0.09807248, -0.02876401, -0.04275325, 0.0032347657, -0.018710755, -0.014923229, -0.043050103, 0.036891624, -0.04443263, 0.017818991, 0.015252669, 0.029773653, -0.025670517, -0.021771709, -0.03774434, -0.020123089, -0.014339159, 0.018985782, 0.00015361028, -0.03646554, 0.0011239746, -0.002032991, -0.036095288, 0.007922519, -0.018507566, 0.014016121, -0.0037247892, -0.0017194182, -0.03272463, -0.010361633, -0.0033071903, -0.016990002, 0.016192831, 0.010987666, 0.009306518, 0.013089006, -0.03140912, -0.011046338, -0.01911865, -0.024897082, 0.028640177, -0.019748366, 0.025198756, -0.0047802073, -0.016916381, -0.00180399, 0.01799536, -0.039696313, -0.052507736, -0.012502296, 0.005870945, -0.022394648, -0.054305885, -0.015182224, 0.02663329, 0.023537612, 0.027149726, -0.017552134, 0.014796113, -0.041574217, 0.030749772, -0.0039778324, 0.02394367, -0.021830657, 0.00553889, 0.0060460926, -0.022031866, 0.017142015, 0.014998016, 0.06859215, 0.03875101, -0.0030290121, -0.048426997, -0.061197076, -0.13152708, 0.0036861827, 0.15746814, -0.002796506, -0.023542495, 0.035596546, -0.0033597543, 0.0210089, -0.005794937, -0.0019467117, 0.023123125, -0.053008772, 0.04418375, -0.026305594, -0.035468172, -0.02759798, -0.007338026, -0.009124798, -0.008948654, -0.011702115, -0.008995221, 0.011158224, -0.012821096, 0.004303045, 0.02190347, -0.005209609, 0.014604443, -0.032478824, 0.006524888, -0.0055398736, 0.003975918, 0.024836114, -0.0041247457, -0.028648343, -0.027908659, 0.009573043, 0.034014784, -0.011021784, -0.023984436, -0.02501188, -0.01978877, 0.0052223383, 0.006285223, -0.036400646, -0.008718276, -0.00039931905, 0.025100159, 0.041325226, -0.0031872084, 0.024866799, -0.045965727, 0.0114272125, 0.043781046, 0.0065883393, -0.047527745, 0.020367006, 0.026299387, -0.0062142983, 0.018613841, -0.0228128, -0.0026361211, -0.00018075842, -0.0763661, -0.016525095, 0.04081546, -0.03973054, 0.0030123012, -0.032965392, 0.00960623, -0.005858072, 0.06439083, -0.06228153, 0.021748757, -0.044164903, 0.056839745, 0.006316062, -0.048006937, -0.044936463, -0.04052175, -0.0043371907, -0.04812758, 0.0213239, 0.01151345, -0.07992748, 0.0343827, -0.046988975, -0.014863918, 0.014869178, 0.005586067, -0.035425548, 0.008991548, 0.030837346, 0.043293137, 0.043413483, -0.059512045, -0.011743517, 0.0077992347, 0.057183284, 0.049263, 0.032663755, -0.0018390435, -0.026532529, 0.017190814, -0.05462443, -0.017952032, 0.025276132, -0.017648533, -0.005231413, 0.046912845, -0.016935112, -0.0029127349, 0.03642388, -0.021214977, 0.024643539, 0.0071802, -0.014923076, 0.001379797, -0.01269743, -0.02582329, -0.028991537, -0.014246563, -0.018708883, 0.0041175666, 0.064451106, -0.020078605, -0.03672293, 0.059672207, -0.01479167, -0.011238005, -0.03247014, 0.009666133, -0.009125713, 0.027650451, 0.012119229, -0.05797259, 0.021469975, -0.006569921, -0.016459016, -0.010162176, 0.26086706, -0.04610037, 0.045995843, -0.022174787, -0.023956548, 0.014110774, -0.011765103, -0.01664091, -0.017658414, -0.0019226134, -0.014441692, 0.05200107, -0.0128434105, -0.06252367, 0.05256512, 0.018301964, 0.0352667, -0.029554319, 0.025350753, -0.0016011023, -0.03625479, 0.036760386, 0.0134843495, -0.020517357, 0.17750381, -0.0013684812, -0.056198064, -0.022435097, 0.006156891, -0.0055269874, 0.0073248376, 0.025760034, -0.07274893, -0.031101352, -0.005838204, 0.004488245, -0.06093529, 0.027619729, -0.02353381, -0.03994621, 0.0047281994, -0.019536775, -0.024118517, 0.054871976, 0.00858913, -0.049041428, 0.017706994, -0.03095488, 0.0060975607, -0.06923297, 0.020672668, 0.016667208, 0.01644976, -0.047804773, 0.021652887, 0.0072762766, 0.002496395, 0.0026595567, 0.03363158, 0.029201934, -0.007592647, -0.04018515, 0.014863427, 0.040121116, 0.00847753, 0.051404838, -0.05068466, 0.040606808, -0.06670301, 0.016778208, 0.024748402, 0.040972542, -0.034444254, 0.00985218, 0.017276347, 0.0007638577, 0.043573845, 0.022153616, -0.02566223, 0.009941818, 0.041344035, -0.035509456, -0.046621397, -0.048692547, 0.021269957, -0.0032294411, -0.020433456, 0.08431802, 0.008602111, -0.012598069, -0.027104376, -0.044147942, -0.08385194, 0.036385644, -0.005721883, 0.038849644, 0.061452966, 0.03917681, 0.0324381, -0.015606153, 0.08061868, 0.063032046, -0.045610107, 0.0089523615, 0.0055016745, -0.033762693, -0.0025634114, 0.039579667, 0.06964165, 0.00011562926, 0.019534007, -0.0068386034, -0.006389894, 0.09862659, -0.03546833, 0.012838419, -0.013400171, 0.033471216, -0.008850871, 0.026325462, 0.008457378, 0.040365055, 0.021251827, 0.006267205, -0.021253284, -0.08738667, -0.008976318, 0.046054758, 0.060975637, -0.009203617, -0.046366952, -0.016796611, -0.014233516, 0.00978021, 0.040445983, -0.006602541, 0.015619479, 0.013132893, 0.023590133, 0.05250703, -0.009372246, 0.015193965, 0.036738396, -0.040662285, -0.009159858, -0.008752783, -0.030557467, 0.011663873, 0.047329094, 0.0075620906, -0.014711704, 0.043588955, -0.021187468, -0.009417285, 0.008149289, -0.0205385, -0.01162251, 0.03645923, -0.07580179, 0.026998049, 0.013905197, 0.0076927915, -0.012243456, -0.036549628, -0.024826871, -0.040791105, 0.083036646, -0.004631659, -0.003854403, 0.034461267, -0.005429239, -0.04439004, 0.0049286573, 0.06160953, -0.012781796, 0.0011066686, 0.032625396, -0.015231192, -0.0724962, -0.044945233, -0.03599679, 0.030108653, 0.02065762, 0.043811623, -0.04499237, -0.046979357, 0.06538509, -0.033033904, -0.0019929158, 0.022869956, 0.003779591, -0.08409844, -0.008658228, -0.013633575, -0.029457968, 0.043696508, 0.008488115, 0.00836361, 0.011222449, -0.059509523, -0.010449093, 0.019729093, 0.25444743, 0.041160002, 0.0003437251, -0.05611264, 0.04430849, -0.018681534, 0.0063927784, 0.003936068, 0.019641971, 0.030158112, 0.020229122, 0.013391519, 0.010163756, -0.0774303, -0.00089108886, -0.017075824, 0.026292069])

    def similar_words(self, context, target_index, max_words=10, include_similarity=False):
        context_v = self.get_context_vector(context, target_index)

        similarity = (self.w.dot(context_v) + 1.0) / 2  # Cosine similarity can be negative, mapping similarity to [0,1]

        count = 0
        similar_words = []
        for i in (-similarity).argsort():
            word = self.index2word[i]
            if np.isnan(similarity[i]) or word not in all_words:
                continue
            similar_words.append((word, similarity[i]))
            count += 1
            if count == max_words:
                break

        if include_similarity:
            return similar_words
        return [x[0] for x in similar_words]

    # context_phrases is a list of dictionaries containing definitions and examples
    # OLD
    # def sorted_similar_contexts(self, target_index, target_context, words_info):
    #     context_lemmas = self.get_context_lemma(target_context)
    #     target_lemma = context_lemmas[target_index]
    #
    #     target_v = self.get_context_vector(target_context, target_word=target_lemma)
    #
    #     scores = {}
    #     for index, def_examples in enumerate(words_info["details"]):
    #         examples_sum = 0
    #         for example in def_examples["examples"]:
    #             example_v = self.get_context_vector(example, target_word=words_info["word"])
    #             examples_sum += self.mult_sim(target_v, example_v)
    #         if len(def_examples["examples"]) == 0:
    #             scores[index] = 0
    #         else:
    #             scores[index] = examples_sum / len(def_examples["examples"])
    #
    #     # sort array of context phrases (definition and example dicitonaries) and return them in a list
    #     return [words_info["details"][i] for i, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)]

    # context_phrases is a list of dictionaries containing definitions and examples
    # gets average vector for example phrases and determines the similarity with the target phrases
    def sorted_similar_contexts(self, target_index, target_context, words_info):
        context_lemmas = self.get_context_lemma(target_context)
        target_lemma = context_lemmas[target_index]

        target_v = self.get_context_vector(target_context, target_word=target_lemma)

        scores = {}
        for index, def_examples in enumerate(words_info["details"]):
            example_vectors = []
            for example in def_examples["examples"]:
                example_vectors.append(self.get_context_vector(example, target_word=words_info["word"]))
            if len(example_vectors) == 0:
                scores[index] = 0
            else:
                average_vector = np.mean(example_vectors, axis=0)
                scores[index] = self.mult_sim(target_v, average_vector)

        # sort array of context phrases (definition and example dicitonaries) and return them in a list
        return [words_info["details"][i] for i, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)]

    def mult_sim(self, vector1, vector2):
        vector1_similarity = self.w.dot(vector1)
        vector1_similarity[vector1_similarity < 0] = 0.0

        vector2_similarity = self.w.dot(vector2)
        vector2_similarity[vector2_similarity < 0] = 0.0
        return vector1_similarity @ vector2_similarity

    def get_context_lemma(self, context):
        context_list = context.split()
        lemmas = []
        for (word, pos) in nltk.pos_tag(context_list):
            lemmas.append(self.lemmatizer.lemmatize(word, pos=nltk_to_wordnet_pos(pos)))
        return lemmas

    def get_context_vector(self, context, target_index=-1, target_word=None):
        context_list = context.lower().split()

        if target_word is not None:
            context_lemmas = self.get_context_lemma(context)
            try:
                target_index = context_lemmas.index(target_word)
                context_list[target_index] = None
            except:
                print("Error generating context vector with a target_index={} for: {}".format(target_index, context))

        context_v = self.model.context2vec(context_list, target_index)
        return context_v / np.sqrt((context_v * context_v).sum())