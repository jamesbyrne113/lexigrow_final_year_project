{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from data_preparation.word_difficulty_dataset_generator import WordDifficultyData\n",
    "from data_preparation.tf_idf import TfIdfGenerator\n",
    "\n",
    "from common.word_difficulty_classifier import WordDifficultyClassifier\n",
    "from common.model_accuracy import model_accuracy\n",
    "from common.wdd_manager import WDDManager\n",
    "\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paths to correct paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdds_path = \"../data/wdds\" # Path to folder containing data sets\n",
    "preprocessed_data_path = \"../data/reddit_data_processed\" # Path to reddit preprocessed data\n",
    "model_save_path = \"word_difficulty_model.joblib\" #Â Path to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tf, min max scaled...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "wdd_manager = WDDManager(wdds_path)\n",
    "wdd = wdd_manager.get_wdd(scale_type=\"scaled\", data_type=\"tf\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(criterion=\"gini\"), \n",
    "    max_samples=0.8, \n",
    "    n_estimators=710)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wdd.features()\n",
    "y = wdd.output()\n",
    "\n",
    "model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Feature Values for Entire Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get TF values for original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_generator = TfIdfGenerator(preprocessed_data_path, comments_key=\"comment_processed_no_spell_corrections\")\n",
    "tf_idf_generator.compute_tf()\n",
    "tf_values = tf_idf_generator.tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change form of Data Set with a list of the 59 values for each word and a word index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = {}\n",
    "tf_word_values = []\n",
    "for index, word in enumerate(list(tf_values.values())[0].keys()):\n",
    "    word_values = []\n",
    "    for doc in tf_values.keys():\n",
    "        value = tf_values[doc][word]\n",
    "        if math.isnan(value):\n",
    "            value = 0\n",
    "        word_values.append(value)\n",
    "    tf_word_values.append(word_values)\n",
    "    word_indexes[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Unscaled TF CEFR data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tf, no change...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "wdd_manager = WDDManager(\"../\")\n",
    "unscaled_wdd = wdd_manager.get_wdd(scale_type=\"no_change\", data_type=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit MinMaxScaler to the Unscaled CEFR TF values and then Scale the entire TF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(unscaled_wdd.features())\n",
    "scaled_data = scaler.transform(tf_word_values)\n",
    "\n",
    "scaled_word_frequencies = {}\n",
    "for word, word_index in word_indexes.items():\n",
    "    scaled_word_frequencies[word] = scaled_data[word_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Class that predicts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdc = WordDifficultyClassifier(model, scaled_word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_difficulty_classifier.joblib']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(wdc, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
